{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is the final project of my Udacity Nanodegree as Data Scientist. Goal of the project is to predict user churn of an online music streaming service using machine learning algorithms. \n",
    "\n",
    "Basis of the project is 12 GB dataset containing all information about user interactions with the online streaming service. The data is stored in a AWS Simple Storage Service (S3) bucket in JSON format.\n",
    "Datasets of such a large scale are challenging to process on a single computer and can therefore be referred to as big data. \n",
    "\n",
    "Apache Spark is a popular tool for large scale data processing and will be used to work with the full dataset. It allows to efficiently spread data and computations across a network of distributed computers, called clusters. Each cluster has nodes (computers) that do the computations in parallel.\n",
    "\n",
    "It is best practice to explore the data using a smaller subset to reduce necessary computation. This workspace contains a tiny subset (128MB) of the full dataset available. Functions of the Spark SQL library will serve to find features in the data via descriptive statistics. After finding those features in the dataset needed to predict user churn, the next step is to create a supervised machine learning model with Spark ML first based on the small subset and later the full dataset. \n",
    "\n",
    "The full dataset will be processed in Amazon Web Services (AWS) with an Elastic Map Reduce (EMR) cluster of 3 m5.xlarge machines. \n",
    "\n",
    "The project is structured following CRISP-DM, the Cross Industry Process for Data Mining. These are the steps:\n",
    "\n",
    "- [1. Business Understanding](#BU)\n",
    "- [2. Data Understanding](#DU)\n",
    "- [3. Prepare Data](#prepare)\n",
    "- [4. Data Modeling](#modeling)\n",
    "- [5. Evaluate the Results](#evaluate)\n",
    "- [6. Deploy](#deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries, create Spark session and import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for Spark Session\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# import libraries for sql actions\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import isnan, count, when, col, asc, desc, udf, col, sort_array, avg, countDistinct, datediff\n",
    "#from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, StringType, IntegerType\n",
    "\n",
    "# other libraries\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Spark ML libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler, StringIndexer, Normalizer, PCA, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1610457312368'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'Sparkify'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.port', '64232'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.178.21'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view information about spark configuration\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path and load data\n",
    "path = 'data/mini_sparkify_event_data.json'\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BU'></a>\n",
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparkify is a (fictional) music streaming service with a business case similar to Spotify. The service can be used on two levels, free tier or premium tier. Both levels of service generate revenue for Sparkify. The free tier service is financed by advertisement in between the songs, the premium tier consists of a monthly or yearly subscription fee to have an advertisement free experience. \n",
    "At any moment the user can decide to downgrade from paid/premium to free, upgrade from free to paid/premium or to cancel from the service completely.\n",
    "    \n",
    "Customer churn is when a customer unsubscribes from a service, ceases to purchase a product or stops engaging with a service [[1](https://www.retentionscience.com/blog/why-measuring-your-customer-churn-rate-increases-revenue/)]. In the case of the music streaming service of this project churn will be defined as when a user lands on the 'Cancellation Confirmation' page, which can happen for both paid and free tier users.\n",
    "\n",
    "Usually it is more expensive for a business to acquire new customers than retaining existing customers. Statistics vary from industry to industry, but research indicates that it may cost up to 5 times more to acquire a new customer than to keep an existing one [[2](https://www.forbes.com/sites/jiawertz/2018/09/12/dont-spend-5-times-more-attracting-new-customers-nurture-the-existing-ones/?sh=36d7a6225a8e)].\n",
    "\n",
    "To prevent churn candidates special discounts or other costly measures are offered to customers. These measures usually lower the revenue per customer. The goal is to identify users who are about to churn ahead of time and only target them with marketing campaigns. \n",
    "\n",
    "Churn prediction is an important classification use case for streaming services such as Netflix, Spotify or Apple Music. Companies that can predict customers who are about to churn ahead of time can implement a more effective customer retention strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DU'></a>\n",
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to understand the length of the dataset and the available columns of the data subset (128MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names and datatypes of object\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first row\n",
    "user_log.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length of dataset\n",
    "print('The dataset contains {} rows'.format(user_log.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'artist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique artists in dataset\n",
    "unique_artists = user_log.select('artist').dropDuplicates().count()\n",
    "\n",
    "print('There are {} unique artists in the dataset.'.format(unique_artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list artists by occurance\n",
    "user_log.select(['artist', 'song']).groupby('artist').count().orderBy('count', ascending=False).show(n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common value for the artist column in the dataset is 'null'. If the rows containing 'null' as value are valid has to be investigated further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'auth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list unique values in authentication column by occurances\n",
    "user_log.select(['auth', 'userId']).groupby('auth').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into sample rows of authentication status guest\n",
    "user_log.filter('auth = \"Guest\"').select(\n",
    "    'artist', 'auth', 'firstName', 'method', 'page', 'status', 'userId', 'sessionId').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to interact with Sparkify as a guest. These rows can be dropped, as it is not possible to assign the user interactions of guests to a userId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into sample rows of authentication status guest\n",
    "user_log.filter('auth = \"Cancelled\"').select(\n",
    "    'artist', 'auth', 'firstName', 'method', 'page', 'status', 'userId', 'sessionId').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows with authentication cancelled seem to be valid. It appears as if the authentification status 'cancelled' appears, when a user lands on the 'Cancellation Confirmation' page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns 'gender' and 'userId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique possible values of column 'gender' and occurances\n",
    "user_log.select(['gender', 'userId']).groupby('gender').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows the count of female/male/null entried in ther gender column for all rows. To see how many users are actually of which gender this table has to be filtered to unique userIds. This will help to answer if the dataset is balanced regarding gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(['gender', 'userId']) \\\n",
    "    .groupby('gender') \\\n",
    "    .agg(F.countDistinct('userId')) \\\n",
    "    .orderBy('count(userId)').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 226 unique userIds (225 valid) in the dataset. \n",
    "Of those users 121 are male and 104 are female users. \n",
    "There are 0 userId's where the gender is not known\n",
    "There is only a slight imbalance regarding gender of users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns 'itemInSession', 'sessionId', 'song', 'ts'  and 'length'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at behaviour of values in itemInSession for one user as example\n",
    "user_log.select(['itemInSession', 'userId', 'sessionId', 'ts', 'page', 'song', 'firstName', 'length']) \\\n",
    "    .filter(user_log.userId == \"18\") \\\n",
    "    .show(n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in column 'itemInSession' counts the interactions which happend for one user during the same sessionId. The value in column 'length' describes the duration of time a song was played and therefore is null for all  page events other than 'NextSong'. The column 'ts' stands for timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique possible values of column 'level'\n",
    "user_log.select(['level', 'userId']) \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupby('level').count() \\\n",
    "    .orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 196 unique userIds of free tier and 166 of paid tier users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'location'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique possible values of column 'location'\n",
    "user_log.select('userId', 'location') \\\n",
    "    .dropDuplicates() \\\n",
    "    .sort('location').show(n=8, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the highest count of unique locations per userId\n",
    "user_log.select(['userId', 'location']) \\\n",
    "    .groupby('userId').agg(F.countDistinct('location')) \\\n",
    "    .orderBy('count(location)', ascending=False).show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset users always access the streaming service from the same location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique possible values of column 'method'\n",
    "user_log.select(['method', 'userId']) \\\n",
    "    .groupby('method').count() \\\n",
    "    .orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique possible values of column 'page'\n",
    "page_events = user_log.select(['page', 'userId']) \\\n",
    "    .groupby('page').count() \\\n",
    "    .withColumnRenamed('count', 'page_event_count') \\\n",
    "    .orderBy('count', ascending=False)\n",
    "\n",
    "page_events.show(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pyspark df to pandas df\n",
    "page_events_pd = page_events.toPandas()\n",
    "\n",
    "# plot graph for distribution\n",
    "barplot = sns.barplot(x = 'page_event_count', y = 'page', data=page_events_pd, palette = 'dark')\n",
    "plt.title('State Distribution of Users', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most occured page event is 'NextSong', which is the main function of the streaming service. It seems like the 'NextSong' page get loaded automatically once a song ends. The Home page is the page the user enters when starting a streaming session and the second most common called page event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'registration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at behaviour of values in the column registration for one user as example\n",
    "user_log.select(['userId', 'registration']) \\\n",
    "    .groupby('userId') \\\n",
    "    .agg(F.countDistinct('registration')) \\\n",
    "    .orderBy('count(registration)', ascending=False) \\\n",
    "    .show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ordering the amount of registrations per user descending from highest amount per user, it is visible that ther is no userId with more than one registration. Therefore the conclusion is that every userId has exactly one registration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column 'userAgent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique possible values of column 'userAgent'\n",
    "user_log.select('userAgent').dropDuplicates().show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column user agent describes the software used to access the streaming service. Examples are Mac OS X with Safari, Windows or Ubuntu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### column ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_convert_ts = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(\n",
    "    timestamp / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "min_ts = user_log.agg({'ts':'min'}) \n",
    "max_ts = user_log.agg({'ts':'max'})\n",
    "\n",
    "min_ts.withColumn('first_date', udf_convert_ts('min(ts)')).show()\n",
    "max_ts.withColumn('last_date', udf_convert_ts('max(ts)')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed time frame is from 2018-10-01 to 2018-12-03."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare'></a>\n",
    "## 3. Prepare Data\n",
    "\n",
    "After getting a Data Understanding, the next step of the CRISP-DM is to prepare the dataset. \n",
    "The first step of data preparation in this project is cleaning the data from invalid or missing data - for example, records without userids or sessionids. \n",
    "After that an exploratory data analysis will be conducted to find possible features for the customer churn prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Clean Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change format of timestamp to human readable\n",
    "\n",
    "udf_convert_ts_to_datetime = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(\n",
    "    timestamp / 1000.0).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "udf_convert_ts_to_date = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(\n",
    "    timestamp / 1000.0).strftime('%Y-%m-%d'))\n",
    "\n",
    "user_log = user_log.withColumn('datetime', udf_convert_ts_to_datetime(user_log.ts)) \\\n",
    "    .withColumn(\"date\", udf_convert_ts_to_date(user_log.ts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any row/record where there is no userId or sessionId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataset contains {} rows before cleaning'.format(user_log.count()))\n",
    "\n",
    "# drop any record with NANs in user ID OR session ID and save to new object\n",
    "user_log_valid = user_log.dropna(how = 'any', subset = ['userId', 'sessionId'])\n",
    "\n",
    "print('The dataset contains {} rows after dropping any NA value in userId and sessionId'.format(\n",
    "    user_log_valid.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there are no missing values in the userId or sessionId columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show unique user IDs\n",
    "user_log.select('userId').dropDuplicates().sort('userId').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are userId values that are empty strings. These empty userIds appear for example when a user enters the streaming service without logging in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty userIds\n",
    "user_log_valid = user_log_valid.filter(user_log_valid['userId'] != '')\n",
    "\n",
    "print('The dataset contains {} rows after filtering out empty userIds'.format(\n",
    "    user_log_valid.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if guest entries are dropped all\n",
    "user_log_valid.select(['auth', 'userId']).groupby('auth').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate amount of users dropped\n",
    "amount_users_dropped = user_log.count() - user_log_valid.count()\n",
    "amount_logged_out = user_log.select('auth').filter('auth = \"Logged Out\"').count()\n",
    "amount_guest = user_log.select('auth').filter('auth = \"Guest\"').count()\n",
    "amount_rows = user_log_valid.count()\n",
    "\n",
    "print('{} Rows without userId were dropped. This were the users with authentification status \"Logged Out\" ({}) \\\n",
    "and \"Guest\" ({}). Resulting into {} rows in the cleaned dataset.'.format(\n",
    "    amount_users_dropped, amount_logged_out, amount_guest, amount_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exploratory Data Analysis\n",
    "\n",
    "Goal of the following exploratory data analysis is to observe differences in the behavior of customers who stayed vs customers who churned. \n",
    "One way is to explore aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Churn\n",
    "\n",
    "First a column 'churned' will be created to use as label to differentiate between customers who churned and those who stayed with the service. \n",
    "This column will be used later by the supervised machine learning model as label to train.\n",
    "The 'Cancellation Confirmation' events serve to define the exact moment of churn, which happens for both paid and free users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show records where Cancellation was confirmed (churn)\n",
    "user_log_valid.filter(\"page = 'Cancellation Confirmation'\") \\\n",
    "    .select('auth', 'firstName', 'gender', 'itemInSession', 'level', 'location', 'userId') \\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find users who had event 'Cancellation Confirmation'\n",
    "user_log_valid.select('userID').where(user_log.page == 'Cancellation Confirmation').show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect example funnel with event 'Cancellation Confirmation' (without most common event NextSong event)\n",
    "user_log_valid.select(['userId', 'level', 'sessionId', 'page']) \\\n",
    "    .where((user_log_valid.userId == '18') & (user_log_valid.page != 'NextSong')) \\\n",
    "    .tail(num=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined function to flag the events page='Cancellation Confirmation' with extra column\n",
    "flag_cancellation_event = F.udf(\n",
    "    lambda cancellation_event: 1 if cancellation_event=='Cancellation Confirmation' else 0, IntegerType())\n",
    "\n",
    "# create extra column 'Cancellation Confirmation'\n",
    "user_log_valid = user_log_valid.withColumn('cancellation_event', flag_cancellation_event('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if column 'cancellation_event' is created correctly\n",
    "user_log_valid.select(['userId', 'level', 'sessionId', 'page', 'cancellation_event']) \\\n",
    "    .where((user_log_valid.userId == '18') & (user_log_valid.page != 'NextSong')) \\\n",
    "    .tail(num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new column 'cancellation_event' was created to mark the exact event of cancellation confirmation. As the goal of this project is to predict users who eventually churn, it is important to find the features which describe best the differences between users who churn and those who not.\n",
    "There is another column necessary to compare all user intereactions made by users who eventually churned to users who did not churn. The values of the new column 'churned users' are true if the user will eventually churn and false if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column with churned users = true\n",
    "churned_users = user_log_valid.select('userId') \\\n",
    "    .filter(user_log.page == 'Cancellation Confirmation') \\\n",
    "    .dropDuplicates().collect()\n",
    "\n",
    "userid_churn = []\n",
    "for i in churned_users:\n",
    "    userid_churn.append(i[0])\n",
    "    \n",
    "user_log_valid = user_log_valid \\\n",
    "    .withColumn('churned',user_log_valid['userId'].isin(userid_churn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if column 'churned' is filled correctly\n",
    "user_log_valid.select(['userId', 'level', 'page', 'cancellation_event', 'churned']) \\\n",
    "    .where((user_log_valid.userId == '18') & (user_log_valid.page != 'NextSong')) \\\n",
    "    .tail(num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new column 'churned' it is possible to check, if the dataset it balanced regarding the number of users who eventually churn and those who stay. The churn rate for ther observed period can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataset is balanced regarding churned users\n",
    "users = user_log_valid.select('userId').dropDuplicates().count()\n",
    "churned_users = user_log_valid.filter('churned = True').select('userId').dropDuplicates().count()\n",
    "stayed_users = user_log_valid.filter('churned = False').select('userId').dropDuplicates().count()\n",
    "\n",
    "print('Of total {} users, {} users stayed with the streaming service during the observed time and \\\n",
    "{} users eventually churned (churn rate: {:2.2f}% ).'.format(users, stayed_users, churned_users, \n",
    "                                                         churned_users/users*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a imbalance in the dataset regarding the amount of users who churned versus those who stayed. Next step is to check how this imbalance in amount of users scales on the amount of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interactions = user_log_valid.count()\n",
    "churned_users_interactions = user_log_valid.filter('churned = True').count()\n",
    "stayed_users_interactions = user_log_valid.filter('churned = False').count()\n",
    "\n",
    "print('Of total {} user interactions, {} user interactions were recorderd of useres who stayed with the \\\n",
    "streaming service during the observed timer and {} user interactions were recorded of users who eventually \\\n",
    "churned ({:2.2f}% of user interaction are by churned customers).'.format(user_interactions, churned_users_interactions, stayed_users_interactions,\n",
    "                                    churned_users_interactions/user_interactions*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of data available regarding interactions to analyze the difference in behaviour for users who stayed versus users who churned is clearly imbalanced. \n",
    "\n",
    "Imbalance in the training data can lead to naive behaviour in the prediction of the supervised machine learning model. With 76.89% of users not churning a prediction accuracy of 76.89% can be achieved by simply always predicting 'no churn' [3](https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28).\n",
    "\n",
    "There are different ways to handle imbalanced data before feeding it to machine learning algorithms. \n",
    "One way would be to manipulate the input data by eiter undersampling data of loyal users, oversampling the data of churned users or generating synthetic data.\n",
    "In this project the way to handle the imbalance in data of interaction with the service by churned users will be creating additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate days since registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total number of days since registration\n",
    "user_log_valid = user_log_valid.withColumn('days_since_registration', F.ceil(\n",
    "    (user_log_valid.ts - user_log_valid.registration)/(1000*60*60*24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Statistics by Hour\n",
    "\n",
    "In the next few cells will be calculated if there amount of songs played by users differs during the hours of a day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user defined function to convert the ts column into hour format\n",
    "get_hour = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(timestamp / 1000.0). hour)\n",
    "\n",
    "# create new column \"hour\" by applying udf get_hour\n",
    "user_log_valid = user_log_valid.withColumn('hour', get_hour(user_log_valid.ts))\n",
    "\n",
    "# take only amount of action \"NextSong\" and group it by hour of day\n",
    "songs_in_hour = user_log_valid.filter(user_log_valid.page == 'NextSong').groupby(\n",
    "    'hour').count().orderBy(user_log_valid.hour.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spark object ot pandas dataframe\n",
    "songs_in_hour_pd = songs_in_hour.toPandas()\n",
    "songs_in_hour_pd.hour = pd.to_numeric(songs_in_hour_pd.hour)\n",
    "\n",
    "# plot the amount of songs played over hours of day\n",
    "plt.scatter(songs_in_hour_pd['hour'], songs_in_hour_pd['count'])\n",
    "plt.xlim(-1, 24);\n",
    "plt.ylim(0, 1.2 * max(songs_in_hour_pd['count']))\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Songs played');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Amount of Page Events per UserId\n",
    "\n",
    "How many page events happen per userId for the observed time period in total comparing churned with loyal users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total amount of page events per userId\n",
    "page_event_per_user = user_log_valid.select('page','userId', 'churned') \\\n",
    "    .groupby('userId', 'churned').count() \\\n",
    "    .withColumnRenamed('count', 'total_page_events') \\\n",
    "\n",
    "page_event_per_user.orderBy('total_page_events', ascending=False).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of page events per user\n",
    "page_event_per_user.groupby('churned').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spark object to pandas dataframe\n",
    "page_event_per_user_pd = page_event_per_user.toPandas()\n",
    "\n",
    "# Plot\n",
    "ax = sns.violinplot(data=page_event_per_user_pd, y='churned', x='total_page_events', orient='h')\n",
    "plt.xlabel('Page Events')\n",
    "plt.ylabel('Customer Evantually Churned')\n",
    "plt.title('Page Events per User Churned versus Loyal')\n",
    "sns.despine(ax=ax);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing users who churned with those who stayed, there is clearly a difference in mean page events per user. It seems plausible that, in average, users who do not churn might stay longer with the service and therfore as well have more page events. \n",
    "More interesting would be if users have more page events per time period.\n",
    "In the following cells will be investigated if there is a difference in page events per session as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to create Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistic_per_userId_streamingTime(pyspark_df, event_type):\n",
    "    '''\n",
    "    Function to calculate amount of certain events per userId.\n",
    "    \n",
    "    args:\n",
    "        pyspark_df - (pyspark dataframe) user_log of Sparkify\n",
    "        event - (string) page event\n",
    "    return:\n",
    "    '''\n",
    "    streaming_time_df = pyspark_df.filter(pyspark_df.page == 'NextSong') \\\n",
    "        .groupBy('userId', 'churned') \\\n",
    "        .agg((F.sum('length')/3600).alias('streamingTime_h'))\n",
    "        #(F.count('length').alias('streamingTime_h')))\n",
    "        #\n",
    "\n",
    "    event_statistics_df = pyspark_df.filter(pyspark_df.page == event_type) \\\n",
    "        .groupBy('userId') \\\n",
    "        .agg(F.count('userId').alias(event_type))\n",
    "\n",
    "    feature_df = streaming_time_df.join(event_statistics_df, on=['userId'], how='inner')\n",
    "\n",
    "    feature_df = feature_df.withColumn(\n",
    "        event_type+'_per_streamingTime', feature_df[event_type]/feature_df.streamingTime_h)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_events_per_userStreaming(feature_pyspark_df, event_type):\n",
    "    '''\n",
    "    Function to create boxplot of pyspark dataframe created with 'count_events_per_user' function.\n",
    "    \n",
    "    args:\n",
    "        count_pysparkdf - (pyspark dataframe) created with 'count_events_per_user'\n",
    "        event - (string) page event\n",
    "    \n",
    "    return:\n",
    "    '''\n",
    "    feature_pyspark_pd = feature_pyspark_df.toPandas()\n",
    "    \n",
    "    ax = sns.violinplot(data=feature_pyspark_pd, y='churned', x=event_type, orient='h')\n",
    "    plt.xlabel(event_type + ' per UserId')\n",
    "    plt.ylabel('Customers Churned')\n",
    "    plt.title(event_type + ' per UserId per StreamingTime churned/not churned')\n",
    "    sns.despine(ax=ax);\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_events_per_user(feature_pyspark_df, event_type):\n",
    "    '''\n",
    "    Function to create boxplot of pyspark dataframe created with 'count_events_per_user' function.\n",
    "    \n",
    "    args:\n",
    "        count_pysparkdf - (pyspark dataframe) created with 'count_events_per_user'\n",
    "        event - (string) page event\n",
    "    \n",
    "    return:\n",
    "    '''\n",
    "    feature_pyspark_pd = feature_pyspark_df.toPandas()\n",
    "    \n",
    "    ax = sns.violinplot(data=feature_pyspark_pd, y='churned', x=event_type, orient='h')\n",
    "    plt.xlabel(event_type)\n",
    "    plt.ylabel('Customers Churned')\n",
    "    plt.title(event_type + ' per UserId churned/not churned')\n",
    "    sns.despine(ax=ax);\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_events_lastXsessions(pyspark_df, amount_sessions):\n",
    "    '''\n",
    "    Function to calculate mean of all events per userId for last x sessionIds.\n",
    "    \n",
    "    args:\n",
    "        pyspark_df - (pyspark dataframe) user_log of Sparkify\n",
    "        amount_sessions - amount of last sessions from which mean is calculated\n",
    "        event_type - (string) page event\n",
    "    return:\n",
    "    '''\n",
    "    \n",
    "    user_window = Window \\\n",
    "        .partitionBy('userID') \\\n",
    "        .orderBy(F.desc('max(ts)')) \n",
    "\n",
    "    pyspark_df = pyspark_df.select('userId', 'churned', 'sessionId', 'page', 'ts') \\\n",
    "        .groupBy('userId', 'churned', 'sessionId') \\\n",
    "        .agg(F.max('ts'), F.count('page')) \\\n",
    "        .withColumn('session_number', F.row_number().over(user_window))\n",
    "    \n",
    "    pyspark_df = pyspark_df.filter(pyspark_df.session_number <= amount_sessions) \\\n",
    "        .groupBy('userId', 'churned') \\\n",
    "        .agg(F.avg('count(page)')) \\\n",
    "        .withColumnRenamed('avg(count(page))', 'average_amount_songs_lastXsessions')\n",
    "    \n",
    "    return pyspark_df\n",
    "\n",
    "def find_optimal_amount_sessions_for_max_diff(pySpark_df):\n",
    "    '''\n",
    "    Function to find amount of sessions with highest difference in mean.\n",
    "    \n",
    "    args:\n",
    "        pyspark_df - (pyspark dataframe) user_log of Sparkify\n",
    "        event_type - (string) page event\n",
    "    return:\n",
    "    '''\n",
    "    for i in range(1,10):\n",
    "        avg_amount_songs_lastXsessions = calculate_avg_events_lastXsessions(pySpark_df, i)\n",
    "\n",
    "        diff_pd = avg_amount_songs_lastXsessions.groupBy('churned').mean() \\\n",
    "            .select('avg(average_amount_songs_lastXsessions)').toPandas()\n",
    "\n",
    "        print('Difference in average events for last {} sessions is {}'.format(\n",
    "            i, diff_pd.values[1][0] - diff_pd.values[0][0]))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_page_event_lastXsessions(pyspark_df, amount_sessions, event_type):\n",
    "    '''\n",
    "    Function to calculate mean of certain events per userId for last x sessionIds.\n",
    "    \n",
    "    args:\n",
    "        pyspark_df - (pyspark dataframe) user_log of Sparkify\n",
    "        amount_sessions - amount of last sessions from which mean is calculated\n",
    "        event_type - (string) page event\n",
    "    return:\n",
    "    '''\n",
    "    \n",
    "    user_window = Window \\\n",
    "        .partitionBy('userID') \\\n",
    "        .orderBy(F.desc('max(ts)')) \n",
    "\n",
    "    pyspark_df = pyspark_df.select('userId', 'churned', 'sessionId', 'page', 'ts') \\\n",
    "        .filter(pyspark_df.page == event_type) \\\n",
    "        .groupBy('userId', 'churned', 'sessionId') \\\n",
    "        .agg(F.max('ts'), F.count('page')) \\\n",
    "        .withColumn('session_number', F.row_number().over(user_window))\n",
    "    \n",
    "    pyspark_df = pyspark_df.filter(pyspark_df.session_number <= amount_sessions) \\\n",
    "        .groupBy('userId', 'churned') \\\n",
    "        .agg(F.avg('count(page)')) \\\n",
    "        .withColumnRenamed('avg(count(page))', 'average_amount_songs_lastXsessions')\n",
    "    \n",
    "    return pyspark_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_amount_sessions_for_max_diff(pySpark_df, event_type):\n",
    "    '''\n",
    "    Function to find amount of sessions with highest difference in mean.\n",
    "    \n",
    "    args:\n",
    "        pyspark_df - (pyspark dataframe) user_log of Sparkify\n",
    "        event_type - (string) page event\n",
    "    return:\n",
    "    '''\n",
    "    for i in range(1,10):\n",
    "        avg_amount_songs_lastXsessions = calculate_avg_page_event_lastXsessions(pySpark_df, i, event_type)\n",
    "\n",
    "        diff_pd = avg_amount_songs_lastXsessions.groupBy('churned').mean() \\\n",
    "            .select('avg(average_amount_songs_lastXsessions)').toPandas()\n",
    "\n",
    "        print('Difference in average {} events for last {} sessions is {}'.format(\n",
    "            event_type, i, diff_pd.values[1][0] - diff_pd.values[0][0]))\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbs Up Events per UserId per StreamingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate events per user and streaming time\n",
    "thumbsUp_per_streamingTime = create_statistic_per_userId_streamingTime(user_log_valid, 'Thumbs Up')\n",
    "\n",
    "# show table with mean of churned/stayed\n",
    "thumbsUp_per_streamingTime.groupBy('churned').mean() \\\n",
    "    .select('churned', 'avg(Thumbs Up_per_streamingTime)').show()\n",
    "\n",
    "# create plot \n",
    "plot_events_per_userStreaming(thumbsUp_per_streamingTime, 'Thumbs Up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users who eventually churn give in average 37.18 Thumbs Up during usage of the service, users who stay with the service 62.98. To eliminate the influence of average longer registration duration for customers who did not churn, the graphic above shows Thumbs Up Events per UserIs per total streaming time of the user. There is only a slight difference in mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbs Down Events per UserId per StreamingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate events per user and streaming time\n",
    "thumbsDown_per_streamingTime = create_statistic_per_userId_streamingTime(user_log_valid, 'Thumbs Down')\n",
    "\n",
    "# show table with mean of churned/stayed\n",
    "thumbsDown_per_streamingTime.groupBy('churned').mean() \\\n",
    "    .select('churned', 'avg(Thumbs Down_per_streamingTime)').show()\n",
    "\n",
    "# create plot \n",
    "plot_events_per_userStreaming(thumbsDown_per_streamingTime, 'Thumbs Down')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphic above shows Thumbs Down Events per UserIs per total streaming time of the user. The difference in mean of Thumbs Down Events is higher than for Thumbs Up Events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roll Advert Events per UserId per StreamingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate events per user and streaming time\n",
    "rollAdvert_per_streamingTime = create_statistic_per_userId_streamingTime(user_log_valid, 'Roll Advert')\n",
    "\n",
    "# show table with mean of churned/stayed\n",
    "rollAdvert_per_streamingTime.groupBy('churned').mean() \\\n",
    "    .select('churned', 'avg(Roll Advert_per_streamingTime)').show()\n",
    "\n",
    "# create plot \n",
    "plot_events_per_userStreaming(rollAdvert_per_streamingTime, 'Roll Advert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset analyzed the average amount of adverts users experience is significantly higher for those who eventually churn, than for those who stay with the service. It is possible that this is due to a higher percentage of users using the free tier among those who churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.groupBy('churned','level') \\\n",
    "    .agg(F.count('userId')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {:2.2f}% free tier users among those who churned and {:2.2f}% free tier users among those who \\\n",
    "    stayed'.format(12388/(32476+12388)*100,43333/(189957+43333)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downgrade Events per UserId per StreamingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate events per user and streaming time\n",
    "downgrade_per_streamingTime = create_statistic_per_userId_streamingTime(user_log_valid, 'Downgrade')\n",
    "\n",
    "# show table with mean of churned/stayed\n",
    "downgrade_per_streamingTime.groupBy('churned').mean() \\\n",
    "    .select('churned', 'avg(Downgrade_per_streamingTime)').show()\n",
    "\n",
    "# create plot \n",
    "plot_events_per_userStreaming(downgrade_per_streamingTime, 'Downgrade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small difference in mean amount of Downgrade Page visits among User who churned versus not chunred relative to the his/her total streaming time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downgrades per UserId in last x sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find amount of sessions with highest difference in mean\n",
    "find_amount_sessions_for_max_diff(user_log_valid, 'Downgrade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average songs played for the last three sessionIds per userId\n",
    "\n",
    "avg_amount_songs_lastXsessions = calculate_avg_page_event_lastXsessions(user_log_valid, 3, 'Downgrade')\n",
    "\n",
    "avg_amount_songs_lastXsessions.groupBy('churned').mean().show()\n",
    "\n",
    "plot_events_per_user(avg_amount_songs_lastXsessions, 'average_amount_songs_lastXsessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comparing the average amount of Downgrade events in the last observed sessionId for users who churned versus users who did stay with the service, the results seem more intuitive. Still, it is not clear if this feature helps to predict users who eventually churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of days from registration date to last observed event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log entry with max amount days since registration\n",
    "days_since_registration = user_log_valid.groupBy('userId', 'churned') \\\n",
    "        .agg(F.max('days_since_registration').alias('days_since_registration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare difference in mean\n",
    "days_since_registration.groupBy('churned').mean().show()\n",
    "\n",
    "# convert to pandas df to plot\n",
    "days_since_registration_pd = days_since_registration.toPandas()\n",
    "\n",
    "# plot\n",
    "ax = sns.violinplot(data=days_since_registration_pd, y='churned', x='days_since_registration', orient='h')\n",
    "plt.xlabel('Number of days from registration date to last observed event')\n",
    "plt.ylabel('Customers Churned')\n",
    "plt.title('Number of days from registration date to last observed event churned/not churned')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant difference in registration duration comparing churned customers with those who stayed with the service. The mean duration from registration to last interaction with the streaming service is 57.8 days for users who eventually churn and 87.1 days for users who stay with the service. Therefore the time passed since a user registered to the streaming service is feature that can be useful to predict users who are prone to churn. There are some outliers visible where churned is false, these are the users who registered to the streaming service and stopped using it without cancelling the service.\n",
    "The observed time period is about 2 months.\n",
    "A user could be inactive for two month and use the service again.\n",
    "Therefore in this analysis those users who are just inactivte, but do not cancel from the service, will be defined as not churned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average amount songs played between visiting home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer.\n",
    "\n",
    "udf_homevisit = F.udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "\n",
    "user_window = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(F.desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "songs_per_homevisit = user_log_valid.filter((user_log_valid.page == \"NextSong\") | (user_log_valid.page == 'Home')) \\\n",
    "    .select('userID', 'churned', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', udf_homevisit(F.col('page'))) \\\n",
    "    .withColumn('period', F.sum('homevisit').over(user_window))\n",
    "\n",
    "songs_per_homevisit = songs_per_homevisit.filter((songs_per_homevisit.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'churned', 'period') \\\n",
    "    .agg({'period':'count'}) \\\n",
    "    .groupBy('userID', 'churned').mean() \\\n",
    "    .withColumnRenamed('avg(count(period))', 'avg_songs_between_home') \\\n",
    "    .select('userID', 'churned', 'avg_songs_between_home')\n",
    "\n",
    "\n",
    "songs_per_homevisit.groupBy('churned') \\\n",
    "    .agg({'avg_songs_between_home':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show lowest values of average songs between home visit for userIds\n",
    "songs_per_homevisit.orderBy('avg_songs_between_home', ascending=True).show(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spark object ot pandas dataframe\n",
    "songs_per_homevisit_pd = songs_per_homevisit.toPandas()\n",
    "\n",
    "# Plot\n",
    "ax = sns.violinplot(data=songs_per_homevisit_pd, y='churned', x='avg_songs_between_home', orient='h')\n",
    "plt.xlabel('Average amount of songs played between visiting the Home Page')\n",
    "plt.ylabel('Customer Evantually Churned')\n",
    "plt.title('Average Amount Songs played per userId between visiting Home Page')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users who evantually churned did in average play about two songs less between visiting the Home page than users who stayed with the service. Churned users played in average 20.16 songs between visiting the Home page, loyal users played in average 22.316 songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Amount Songs played per Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_amount_songs_played_per_session = user_log_valid.select('userId', 'sessionId', 'churned', 'page') \\\n",
    "    .filter('page = \"NextSong\"') \\\n",
    "    .groupby('userId', 'sessionId', 'churned').count() \\\n",
    "    .select('userId', 'churned', 'count') \\\n",
    "    .groupby('userId', 'churned').mean() \\\n",
    "    .withColumnRenamed('avg(count)', 'avg_amount_songs_played_per_session') \\\n",
    "\n",
    "avg_amount_songs_played_per_session.groupBy('churned').mean().show()\n",
    "\n",
    "plot_events_per_user(avg_amount_songs_played_per_session, 'avg_amount_songs_played_per_session')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page Events per SessionId per UserId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_amount_sessions_for_max_diff(user_log_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average songs played for the last three sessionIds per userId\n",
    "\n",
    "avg_amount_songs_lastXsessions = calculate_avg_events_lastXsessions(user_log_valid, 3)\n",
    "\n",
    "avg_amount_songs_lastXsessions.groupBy('churned').mean().show()\n",
    "\n",
    "plot_events_per_user(avg_amount_songs_lastXsessions, 'average_amount_songs_lastXsessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average amount songs played in last x sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count sessionIds per userId\n",
    "\n",
    "user_log_valid.select('userId', 'sessionId') \\\n",
    "    .groupBy('userId').count() \\\n",
    "    .orderBy('count') \\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no user in the dataset with less than 6 sessions.\n",
    "Next step is to define a function to reuse for calculating average occurances of certain events for a defined amount of sessions per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find amount of sessions with highest difference in mean\n",
    "find_amount_sessions_for_max_diff(user_log_valid, 'NextSong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in average songs played comparing churned users with those who stayed is maximal when taking the average songs played for the last three sessionIds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average songs played for the last three sessionIds per userId\n",
    "\n",
    "avg_amount_songs_lastXsessions = calculate_avg_page_event_lastXsessions(user_log_valid, 3, 'NextSong')\n",
    "\n",
    "avg_amount_songs_lastXsessions.groupBy('churned').mean().show()\n",
    "\n",
    "plot_events_per_user(avg_amount_songs_lastXsessions, 'average_amount_songs_lastXsessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage of days since registration where user was active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_df = user_log_valid.groupBy('userId', 'churned') \\\n",
    "    .agg(F.max('days_since_registration').alias('days_since_registration'), \n",
    "         F.countDistinct('date').alias('days_active'))\n",
    "\n",
    "activity_df = activity_df.filter('days_since_registration >= 1')\n",
    "    \n",
    "activity_df = activity_df \\\n",
    "    .withColumn('percentage_active_days', activity_df.days_active/activity_df.days_since_registration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare difference in mean\n",
    "activity_df.groupBy('churned').mean().show()\n",
    "\n",
    "# convert to pandas df to plot\n",
    "activity_pd = activity_df.toPandas()\n",
    "\n",
    "# plot\n",
    "ax = sns.violinplot(data=activity_pd, y='churned', x='percentage_active_days', orient='h')\n",
    "plt.xlabel('Percentage of days since registration where user was active')\n",
    "plt.ylabel('Customers Churned')\n",
    "plt.title('Percentage of days since registration where user was active churned/not churned')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming time per active day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = user_log_valid.filter(user_log_valid.page == 'NextSong') \\\n",
    "    .groupBy('userId', 'churned') \\\n",
    "    .agg((F.sum('length')/3600).alias('streamingTime_h'),\n",
    "         F.countDistinct('date').alias('days_active'))\n",
    "    \n",
    "streaming_df = streaming_df \\\n",
    "    .withColumn('streaming_per_active_day', streaming_df.streamingTime_h/streaming_df.days_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare difference in mean\n",
    "streaming_df.groupBy('churned').mean().show()\n",
    "\n",
    "# convert to pandas df to plot\n",
    "streaming_pd = streaming_df.toPandas()\n",
    "\n",
    "# plot\n",
    "ax = sns.violinplot(data=streaming_pd, y='churned', x='streaming_per_active_day', orient='h')\n",
    "plt.xlabel('Streaming hours per active day')\n",
    "plt.ylabel('Customers Churned')\n",
    "plt.title('Streaming hours per active day churned/not churned')\n",
    "sns.despine(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring possible features for a prediction model, the next step is to select the features which should be used for the machine learning model to decide if a user will churn or not.\n",
    "If the model will be based on algorithms like Logistic Regression or Linear Regression, the features have to be checked for Multicollinearity. \n",
    "When the features have high correlation and one feature can be predicted from other features there might be Multicollinearity. This can lead to misleading results in the prediction of the label. \n",
    "Decision trees and boosted trees algorithms are immune to multicollinearity. When they decide to split, the tree will choose only one of the perfectly correlated features.\n",
    " [3](https://towardsdatascience.com/why-feature-correlation-matters-a-lot-847e8ba439c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean dataframe\n",
    "\n",
    "def clean_dataset(user_log_df):\n",
    "    '''\n",
    "    Function to clean user_log datase from userIds and sessionIds from NA and empty values.\n",
    "    '''\n",
    "\n",
    "    user_log_clean_df = user_log_df.dropna(how = 'any', subset = ['userId', 'sessionId'])\n",
    "    user_log_clean_df = user_log_clean_df.filter(user_log_clean_df['userId'] != '')\n",
    "\n",
    "    print('The dataset originally contained {} rows. \\nAfter cleaning there are {} rows left.'\n",
    "          .format(user_log_df.count(), user_log_clean_df.count()))\n",
    "    \n",
    "    return user_log_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labeled dataframe\n",
    "\n",
    "def create_churn_label(user_log_clean_df):\n",
    "    '''\n",
    "    Function to create dataframe with numeric labels for churn, gender and level per userId from user_log.\n",
    "    '''\n",
    "    \n",
    "    user_log_flagged_df = flag_cancellation_events(user_log_clean_df)\n",
    "    labeled_data = create_labeled_userIds(user_log_flagged_df)\n",
    "    \n",
    "    return labeled_data\n",
    "     \n",
    "    \n",
    "def flag_cancellation_events(user_log_clean_df):\n",
    "    '''\n",
    "    Function to create new column in user_log where cancellation events are lagged with 1.\n",
    "    '''\n",
    "    \n",
    "    flag_cancellation_event = F.udf(\n",
    "        lambda cancellation_event: 1 if cancellation_event=='Cancellation Confirmation' else 0, IntegerType())\n",
    "    \n",
    "    user_log_flagged_df = user_log_clean_df.withColumn('cancellation_event', flag_cancellation_event('page'))\n",
    "\n",
    "    return user_log_flagged_df\n",
    "\n",
    "\n",
    "def create_labeled_userIds(user_log_flagged_df):\n",
    "    '''\n",
    "    Function to convert user_log to dataframe with one line per userId.\n",
    "    '''\n",
    "    \n",
    "    window_timeOrdered = Window.partitionBy('userId').orderBy(F.desc('ts'), F.desc('itemInSession'))\n",
    "    user_log_flagged_df = user_log_flagged_df.withColumn('rank', F.rank().over(window_timeOrdered))\n",
    "    labeled_df = user_log_flagged_df.select('userId', 'cancellation_event', 'gender', 'level') \\\n",
    "        .filter(user_log_flagged_df.rank == 1)\n",
    "    \n",
    "    labeled_df = convert_to_numeric(labeled_df, 'gender')\n",
    "    labeled_df = convert_to_numeric(labeled_df, 'level')\n",
    "    \n",
    "    return labeled_df\n",
    "    \n",
    "def convert_to_numeric(labeled_data, categegorical_column):  \n",
    "    '''\n",
    "    Function to convert a categorical dataframe to a numerical dataframe.\n",
    "    '''\n",
    "    \n",
    "    indexer = StringIndexer(inputCol=categegorical_column, outputCol=categegorical_column+'Numeric')\n",
    "    indexed_df = indexer.fit(labeled_data).transform(labeled_data).drop(categegorical_column)\n",
    "\n",
    "    return indexed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset originally contained 286500 rows. \n",
      "After cleaning there are 278154 rows left.\n",
      "+------+------------------+-------------+------------+\n",
      "|userId|cancellation_event|genderNumeric|levelNumeric|\n",
      "+------+------------------+-------------+------------+\n",
      "|100010|                 0|          1.0|         1.0|\n",
      "|200002|                 0|          0.0|         0.0|\n",
      "|   125|                 1|          0.0|         1.0|\n",
      "|   124|                 0|          1.0|         0.0|\n",
      "|    51|                 1|          0.0|         0.0|\n",
      "|     7|                 0|          0.0|         1.0|\n",
      "|    15|                 0|          0.0|         0.0|\n",
      "|    54|                 1|          1.0|         0.0|\n",
      "|   155|                 0|          1.0|         0.0|\n",
      "|100014|                 1|          0.0|         0.0|\n",
      "|   132|                 0|          1.0|         0.0|\n",
      "|   154|                 0|          1.0|         1.0|\n",
      "|   101|                 1|          0.0|         0.0|\n",
      "|    11|                 0|          1.0|         0.0|\n",
      "|   138|                 0|          0.0|         0.0|\n",
      "|300017|                 0|          1.0|         0.0|\n",
      "|100021|                 1|          0.0|         1.0|\n",
      "|    29|                 1|          0.0|         0.0|\n",
      "|    69|                 0|          1.0|         0.0|\n",
      "|   112|                 0|          0.0|         1.0|\n",
      "+------+------------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_clean = clean_dataset(user_log)\n",
    "    \n",
    "labeled_df = create_churn_label(user_log_clean)\n",
    "\n",
    "# feature_df = create_features(user_log_clean)\n",
    "\n",
    "# join feature_df and labeled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature dataframe\n",
    "pyspark_df = user_log_valid\n",
    "# tier\n",
    "features = pyspark_df.groupBy('userId').agg((F.))\n",
    "\n",
    "\n",
    "\n",
    "df1.show(3)\n",
    "\n",
    "# Thumbs Up Events per UserId per StreamingTime\n",
    "# Thumbs Down Events per UserId per StreamingTime\n",
    "# Roll Advert Events per UserId per StreamingTime\n",
    "# Downgrade Events per UserId per StreamingTime\n",
    "# Downgrades per UserId in last x sessions\n",
    "# Number of days from registration date to last observed event\n",
    "# Average amount songs played between visiting home\n",
    "# Average Amount Songs played per Session\n",
    "# Page Events per SessionId per UserId\n",
    "# Average amount songs played in last 3 sessions\n",
    "# Percentage of days since registration where user was active\n",
    "# Streaming time per active day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyspark dataframe with userId, churned and features\n",
    "# separate features from labels \n",
    "# use StringIndexer and OneHotEncoder to convert them into numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector use VectorAssembler to convert the newly created numeric features into vectors\n",
    "# scaling\n",
    "# separeate training from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of days since registration\n",
    "\n",
    "# for every user find registration event\n",
    "min_ts = user_log_valid.select('userId', 'churned', 'ts') \\\n",
    "    .groupBy('userId', 'churned') \\\n",
    "    .agg({'ts':'min'})\n",
    "\n",
    "# for every user find last event\n",
    "max_ts = user_log_valid.select('userId', 'churned', 'ts') \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg({'ts':'max'})\n",
    "\n",
    "# for every user calculate time from registration event to last event\n",
    "register_duration = min_ts.join(max_ts, on=['userId'], how='inner')\n",
    "\n",
    "udf_convert_ts = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(\n",
    "    timestamp / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "register_duration = register_duration \\\n",
    "    .withColumn('registration', udf_convert_ts('min(ts)')) \\\n",
    "    .withColumn('last_page_event', udf_convert_ts('max(ts)')) \\\n",
    "    .select('userId', 'churned', 'registration', 'last_page_event') \\\n",
    "\n",
    "register_duration_a = register_duration \\\n",
    "    .withColumn('register_duration', F.datediff(F.col('last_page_event'), F.col('registration')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
